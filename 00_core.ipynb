{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# core\n",
    "\n",
    "> Utilites for building semantic knowledge graphs in rdflib using a fast.ai approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RDFLib Parquet Storage\n",
    "\n",
    "This module provides a fast, efficient way to store and retrieve RDF graphs using Parquet files. It wraps RDFLib's graph functionality with optimized Parquet serialization methods.\n",
    "\n",
    "## Key Features\n",
    "\n",
    "- Save RDF graphs to compressed Parquet files\n",
    "- Load graphs from Parquet with optimized performance\n",
    "- Preserve all RDF semantics (URIs, blank nodes, literals with datatypes)\n",
    "- Memory-efficient batch processing for large graphs\n",
    "- Fluent API for method chaining\n",
    "- Simple integration with existing RDFLib code\n",
    "\n",
    "## Use Cases\n",
    "\n",
    "- Store large knowledge graphs efficiently\n",
    "- Improve load/save times for RDF data\n",
    "- Integrate semantic web data with data science pipelines\n",
    "- Reduce storage requirements for RDF datasets\n",
    "- Enable faster querying through columnar storage benefits\n",
    "\n",
    "This module bridges the gap between semantic web technologies and modern data engineering practices, allowing RDF data to benefit from Parquet's columnar storage format advantages including compression, schema enforcement, and performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is inspired by KGLabs implimentation [Performance analysis of serialization methodsÂ¶](https://derwen.ai/docs/kgl/ex2_0/#performance-analysis-of-serialization-methods) that showed parquet to be a reasonable triple store for local graphs. This also provides some convenience functions for constructing KGs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import rdflib\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "from fastcore.all import *\n",
    "from pathlib import Path\n",
    "from typing import Union, List, Dict, Any, IO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class KnowledgeGraph:\n",
    "    \"RDFLib wrapper with Parquet storage capabilities\"\n",
    "    _COL_NAMES = [\"subject\", \"predicate\", \"object\"]\n",
    "    \n",
    "    def __init__(self, g=None): \n",
    "        self.g = ifnone(g, rdflib.Graph())\n",
    "    \n",
    "    def __len__(self): return len(self.g)\n",
    "    \n",
    "    def __repr__(self): return f\"KnowledgeGraph(triples={len(self)})\"\n",
    "\n",
    "    @delegates(pd.DataFrame.to_parquet)\n",
    "    def save_parquet(self, path, compression=\"snappy\", **kwargs):\n",
    "        \"Save RDF graph to Parquet file\"\n",
    "        rows = [{self._COL_NAMES[i]:o.n3() for i,o in enumerate(triple)} \n",
    "                for triple in self.g]\n",
    "        df = pd.DataFrame(rows, columns=self._COL_NAMES)\n",
    "        df.to_parquet(path, compression=compression, **kwargs)\n",
    "\n",
    "    @delegates(pd.read_parquet)\n",
    "    def load_parquet(self, path, batch_size=100000, **kwargs):\n",
    "        \"Load RDF graph from Parquet file with optimized performance\"\n",
    "        df = pd.read_parquet(path, **kwargs)\n",
    "        total = len(df)\n",
    "        \n",
    "        # Process in batches to handle large graphs\n",
    "        for start in range(0, total, batch_size):\n",
    "            end = min(start + batch_size, total)\n",
    "            batch = df.iloc[start:end]\n",
    "            \n",
    "            # Direct triple creation instead of parsing\n",
    "            triples = []\n",
    "            for _, row in batch.iterrows():\n",
    "                s_str, p_str, o_str = row['subject'], row['predicate'], row['object']\n",
    "                \n",
    "                # Parse subject (URI or blank node)\n",
    "                if s_str.startswith('<') and s_str.endswith('>'):\n",
    "                    s = rdflib.URIRef(s_str[1:-1])\n",
    "                elif s_str.startswith('_:'):\n",
    "                    s = rdflib.BNode(s_str[2:])\n",
    "                else:\n",
    "                    s = rdflib.Literal(s_str)\n",
    "                    \n",
    "                # Parse predicate (always URI)\n",
    "                if p_str.startswith('<') and p_str.endswith('>'):\n",
    "                    p = rdflib.URIRef(p_str[1:-1])\n",
    "                else:\n",
    "                    p = rdflib.URIRef(p_str)\n",
    "                    \n",
    "                # Parse object (URI, blank node, or literal)\n",
    "                if o_str.startswith('<') and o_str.endswith('>'):\n",
    "                    o = rdflib.URIRef(o_str[1:-1])\n",
    "                elif o_str.startswith('_:'):\n",
    "                    o = rdflib.BNode(o_str[2:])\n",
    "                elif o_str.startswith('\"') or o_str.startswith(\"'\"):\n",
    "                    # This is a simplified approach - full N3 parsing is complex\n",
    "                    # For production, consider using rdflib's parser directly\n",
    "                    o = rdflib.Literal(o_str)\n",
    "                else:\n",
    "                    o = rdflib.Literal(o_str)\n",
    "                    \n",
    "                triples.append((s, p, o))\n",
    "            \n",
    "            # Add all triples in one batch\n",
    "            self.g.addN((s, p, o, self.g) for s, p, o in triples)\n",
    "        \n",
    "        return self\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def add(self:KnowledgeGraph, triple):\n",
    "    \"Add a triple to the graph\"\n",
    "    self.g.add(triple)\n",
    "    return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "@patch\n",
    "def remove(self:KnowledgeGraph, triple):\n",
    "    \"Remove a triple from the graph\"\n",
    "    self.g.remove(triple)\n",
    "    return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "@patch\n",
    "def bind_ns(self:KnowledgeGraph, prefix, namespace):\n",
    "    \"Bind a namespace prefix\"\n",
    "    self.g.namespace_manager.bind(prefix, namespace)\n",
    "    return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "@patch\n",
    "def bind_namespaces(self:KnowledgeGraph, ns_dict):\n",
    "    \"Bind multiple namespace prefixes\"\n",
    "    for prefix, uri in ns_dict.items():\n",
    "        ns = rdflib.Namespace(uri)\n",
    "        self.g.namespace_manager.bind(prefix, ns)\n",
    "    return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def query(self:KnowledgeGraph, q):\n",
    "    \"Run a SPARQL query\"\n",
    "    return self.g.query(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def triples(self:KnowledgeGraph, pattern=None):\n",
    "    \"Return triples matching the pattern\"\n",
    "    pattern = ifnone(pattern, (None, None, None))\n",
    "    return list(self.g.triples(pattern))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def __getitem__(self:KnowledgeGraph, pattern):\n",
    "    \"Get triples matching a pattern using [] syntax\"\n",
    "    return list(self.g.triples(pattern))\n",
    "\n",
    "@patch\n",
    "def from_file(self:KnowledgeGraph, path, format=None):\n",
    "    \"Load graph from a file in any RDFLib-supported format\"\n",
    "    if format is None:\n",
    "        format = Path(path).suffix.lstrip('.')\n",
    "    self.g.parse(path, format=format)\n",
    "    return self\n",
    "\n",
    "@patch\n",
    "def to_file(self:KnowledgeGraph, path, format=None):\n",
    "    \"Save graph to a file in any RDFLib-supported format\"\n",
    "    if format is None:\n",
    "        format = Path(path).suffix.lstrip('.')\n",
    "    self.g.serialize(destination=path, format=format)\n",
    "    return self\n",
    "\n",
    "@patch\n",
    "def summary(self:KnowledgeGraph):\n",
    "    \"Print a summary of the graph\"\n",
    "    n_triples = len(self)\n",
    "    n_subjects = len(set(s for s,_,_ in self.g))\n",
    "    n_predicates = len(set(p for _,p,_ in self.g))\n",
    "    n_objects = len(set(o for _,_,o in self.g))\n",
    "    return dict(triples=n_triples, subjects=n_subjects, \n",
    "                predicates=n_predicates, objects=n_objects)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original graph:\n",
      "{'triples': 17, 'subjects': 3, 'predicates': 5, 'objects': 14}\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "# Example: Working with DBpedia data about famous scientists\n",
    "from urllib.request import urlopen\n",
    "\n",
    "# Create our graph\n",
    "kg = KnowledgeGraph()\n",
    "\n",
    "# Load some real data from DBpedia (small subset about scientists)\n",
    "scientist_data = \"\"\"\n",
    "@prefix dbr: <http://dbpedia.org/resource/> .\n",
    "@prefix dbo: <http://dbpedia.org/ontology/> .\n",
    "@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .\n",
    "\n",
    "dbr:Albert_Einstein a dbo:Scientist ;\n",
    "    rdfs:label \"Albert Einstein\" ;\n",
    "    dbo:field dbr:Physics ;\n",
    "    dbo:birthDate \"1879-03-14\"^^<http://www.w3.org/2001/XMLSchema#date> ;\n",
    "    dbo:deathDate \"1955-04-18\"^^<http://www.w3.org/2001/XMLSchema#date> .\n",
    "\n",
    "dbr:Marie_Curie a dbo:Scientist ;\n",
    "    rdfs:label \"Marie Curie\" ;\n",
    "    dbo:field dbr:Chemistry, dbr:Physics ;\n",
    "    dbo:birthDate \"1867-11-07\"^^<http://www.w3.org/2001/XMLSchema#date> ;\n",
    "    dbo:deathDate \"1934-07-04\"^^<http://www.w3.org/2001/XMLSchema#date> .\n",
    "\n",
    "dbr:Alan_Turing a dbo:Scientist ;\n",
    "    rdfs:label \"Alan Turing\" ;\n",
    "    dbo:field dbr:Computer_Science, dbr:Mathematics ;\n",
    "    dbo:birthDate \"1912-06-23\"^^<http://www.w3.org/2001/XMLSchema#date> ;\n",
    "    dbo:deathDate \"1954-06-07\"^^<http://www.w3.org/2001/XMLSchema#date> .\n",
    "\"\"\"\n",
    "\n",
    "# Parse the data\n",
    "kg.g.parse(data=scientist_data, format=\"turtle\")\n",
    "\n",
    "# Show initial summary\n",
    "print(\"Original graph:\")\n",
    "print(kg.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| eval: false\n",
    "\n",
    "# Test the optimized loader with timing\n",
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Generate a larger test graph with more diverse RDF terms\n",
    "def create_test_graph(size=1000):\n",
    "    kg = KnowledgeGraph()\n",
    "    \n",
    "    # Create some namespaces\n",
    "    ex = rdflib.Namespace(\"http://example.org/\")\n",
    "    foaf = rdflib.Namespace(\"http://xmlns.com/foaf/0.1/\")\n",
    "    schema = rdflib.Namespace(\"http://schema.org/\")\n",
    "    \n",
    "    # Bind namespaces\n",
    "    kg.bind_ns(\"ex\", ex)\n",
    "    kg.bind_ns(\"foaf\", foaf)\n",
    "    kg.bind_ns(\"schema\", schema)\n",
    "    \n",
    "    # Generate triples\n",
    "    for i in tqdm(range(size)):\n",
    "        # Subject (mix of URIs and blank nodes)\n",
    "        if i % 10 == 0:\n",
    "            s = rdflib.BNode(f\"node{i}\")\n",
    "        else:\n",
    "            s = ex[f\"entity{i}\"]\n",
    "            \n",
    "        # Add type\n",
    "        kg.add((s, rdflib.RDF.type, ex.Resource))\n",
    "        \n",
    "        # Add string literal\n",
    "        kg.add((s, schema.name, rdflib.Literal(f\"Resource {i}\")))\n",
    "        \n",
    "        # Add numeric literal\n",
    "        kg.add((s, schema.position, rdflib.Literal(i)))\n",
    "        \n",
    "        # Add typed literal\n",
    "        kg.add((s, schema.dateCreated, rdflib.Literal(f\"2023-{(i%12)+1:02d}-{(i%28)+1:02d}\", \n",
    "                                                    datatype=rdflib.XSD.date)))\n",
    "        \n",
    "        # Add language-tagged literal\n",
    "        kg.add((s, schema.description, rdflib.Literal(f\"Description of resource {i}\", \n",
    "                                                     lang=\"en\")))\n",
    "        \n",
    "        # Add relations to other resources\n",
    "        kg.add((s, schema.related, ex[f\"entity{(i+1)%size}\"]))\n",
    "        kg.add((s, schema.related, ex[f\"entity{(i+size//2)%size}\"]))\n",
    "    \n",
    "    return kg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating test graph...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tqdm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m test_size = \u001b[32m1000\u001b[39m  \u001b[38;5;66;03m# Adjust based on your system's capacity\u001b[39;00m\n\u001b[32m      5\u001b[39m start_time = time.time()\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m test_kg = \u001b[43mcreate_test_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m create_time = time.time() - start_time\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCreated graph with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(test_kg)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m triples in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcreate_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m seconds\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 20\u001b[39m, in \u001b[36mcreate_test_graph\u001b[39m\u001b[34m(size)\u001b[39m\n\u001b[32m     17\u001b[39m kg.bind_ns(\u001b[33m\"\u001b[39m\u001b[33mschema\u001b[39m\u001b[33m\"\u001b[39m, schema)\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# Generate triples\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtqdm\u001b[49m(\u001b[38;5;28mrange\u001b[39m(size)):\n\u001b[32m     21\u001b[39m     \u001b[38;5;66;03m# Subject (mix of URIs and blank nodes)\u001b[39;00m\n\u001b[32m     22\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i % \u001b[32m10\u001b[39m == \u001b[32m0\u001b[39m:\n\u001b[32m     23\u001b[39m         s = rdflib.BNode(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mnode\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'tqdm' is not defined"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "#|eval: false\n",
    "\n",
    "# Create and save a test graph\n",
    "print(\"Creating test graph...\")\n",
    "test_size = 1000  # Adjust based on your system's capacity\n",
    "start_time = time.time()\n",
    "test_kg = create_test_graph(test_size)\n",
    "create_time = time.time() - start_time\n",
    "print(f\"Created graph with {len(test_kg)} triples in {create_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving to Parquet...\n",
      "Saved to Parquet in 0.02 seconds\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "#| eval: false\n",
    "\n",
    "# Save to Parquet\n",
    "print(\"\\nSaving to Parquet...\")\n",
    "start_time = time.time()\n",
    "test_kg.save_parquet(\"test_graph.parquet\")\n",
    "save_time = time.time() - start_time\n",
    "print(f\"Saved to Parquet in {save_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading with original method...\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "#| eval: false\n",
    "\n",
    "# Load with original method\n",
    "print(\"\\nLoading with original method...\")\n",
    "kg_orig = KnowledgeGraph()\n",
    "# Replace the optimized method temporarily\n",
    "orig_load_parquet = lambda self, path, **kwargs: (\n",
    "    pd.read_parquet(path, **kwargs).apply(\n",
    "        \n",
    "        lambda row: self.g.parse(data=f\"{row['subject']} {row['predicate']} {row['object']} .\", format=\"ttl\"),\n",
    "        axis=1\n",
    "    ) and self\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original method failed: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n",
      "\n",
      "Loading with optimized method...\n",
      "Loaded 7000 triples in 0.25 seconds\n",
      "\n",
      "Comparison:\n",
      "Original method: inf seconds\n",
      "Optimized method: 0.25 seconds\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "#|eval: false\n",
    "\n",
    "# Time the original method\n",
    "start_time = time.time()\n",
    "try:\n",
    "    orig_load_parquet(kg_orig, \"test_graph.parquet\")\n",
    "    orig_time = time.time() - start_time\n",
    "    print(f\"Loaded {len(kg_orig)} triples in {orig_time:.2f} seconds\")\n",
    "except Exception as e:\n",
    "    print(f\"Original method failed: {e}\")\n",
    "    orig_time = float('inf')\n",
    "\n",
    "# Load with optimized method\n",
    "print(\"\\nLoading with optimized method...\")\n",
    "kg_opt = KnowledgeGraph()\n",
    "start_time = time.time()\n",
    "kg_opt.load_parquet(\"test_graph.parquet\", batch_size=500)\n",
    "opt_time = time.time() - start_time\n",
    "print(f\"Loaded {len(kg_opt)} triples in {opt_time:.2f} seconds\")\n",
    "\n",
    "# Compare results\n",
    "print(\"\\nComparison:\")\n",
    "print(f\"Original method: {orig_time:.2f} seconds\")\n",
    "print(f\"Optimized method: {opt_time:.2f} seconds\")\n",
    "if orig_time != float('inf'):\n",
    "    print(f\"Speedup: {orig_time/opt_time:.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Verifying data integrity...\n",
      "â Triple count matches: 7000\n",
      "Type triples: original=1000, loaded=1000\n",
      "Date literals: original=1000, loaded=1000\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "#|eval: false\n",
    "\n",
    "# Verify data integrity\n",
    "print(\"\\nVerifying data integrity...\")\n",
    "# Check triples count\n",
    "if len(test_kg) == len(kg_opt):\n",
    "    print(f\"â Triple count matches: {len(test_kg)}\")\n",
    "else:\n",
    "    print(f\"â Triple count mismatch: original={len(test_kg)}, loaded={len(kg_opt)}\")\n",
    "\n",
    "# Run a few sample queries to verify correctness\n",
    "q = \"\"\"\n",
    "SELECT (COUNT(*) as ?count) WHERE {\n",
    "    ?s a ?type .\n",
    "}\n",
    "\"\"\"\n",
    "orig_count = list(test_kg.query(q))[0][0].value\n",
    "opt_count = list(kg_opt.query(q))[0][0].value\n",
    "print(f\"Type triples: original={orig_count}, loaded={opt_count}\")\n",
    "\n",
    "# Check literal handling\n",
    "q = \"\"\"\n",
    "SELECT (COUNT(*) as ?count) WHERE {\n",
    "    ?s <http://schema.org/dateCreated> ?date .\n",
    "}\n",
    "\"\"\"\n",
    "orig_count = list(test_kg.query(q))[0][0].value\n",
    "opt_count = list(kg_opt.query(q))[0][0].value\n",
    "print(f\"Date literals: original={orig_count}, loaded={opt_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating test graph with 5000 entities...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb9ada297c3a4e2088998203817c45e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph contains 35000 triples\n",
      "\n",
      "--- Parquet ---\n",
      "Save time: 0.10s\n",
      "File size: 0.30 MB\n",
      "Load time: 0.95s\n",
      "Loaded 35000 triples\n",
      "\n",
      "--- Turtle ---\n",
      "Save time: 0.45s\n",
      "File size: 1.24 MB\n",
      "Load time: 0.83s\n",
      "Loaded 35000 triples\n",
      "\n",
      "--- Performance Comparison ---\n",
      "Save speedup: 4.58x faster with Parquet\n",
      "Load speedup: 0.88x faster with Parquet\n",
      "Size reduction: 4.13x smaller with Parquet\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Parquet': {'save_time': 0.09809494018554688,\n",
       "  'load_time': 0.9470059871673584,\n",
       "  'file_size': 314087,\n",
       "  'triples': 35000},\n",
       " 'Turtle': {'save_time': 0.4496300220489502,\n",
       "  'load_time': 0.8317821025848389,\n",
       "  'file_size': 1298079,\n",
       "  'triples': 35000}}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "#|eval: false\n",
    "\n",
    "# Benchmark: Parquet vs Turtle file storage\n",
    "import time\n",
    "import os\n",
    "\n",
    "def benchmark_formats(graph_size=10000):\n",
    "    print(f\"Creating test graph with {graph_size} entities...\")\n",
    "    kg = create_test_graph(graph_size)\n",
    "    triples = len(kg)\n",
    "    print(f\"Graph contains {triples} triples\")\n",
    "    \n",
    "    # Benchmark saving\n",
    "    formats = {\n",
    "        \"Parquet\": {\"save\": lambda: kg.save_parquet(\"test_graph.parquet\"), \n",
    "                   \"load\": lambda: KnowledgeGraph().load_parquet(\"test_graph.parquet\")},\n",
    "        \"Turtle\": {\"save\": lambda: kg.to_file(\"test_graph.turtle\", format=\"turtle\"), \n",
    "                  \"load\": lambda: KnowledgeGraph().from_file(\"test_graph.turtle\", format=\"turtle\")}\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for fmt_name, actions in formats.items():\n",
    "        # Test save\n",
    "        print(f\"\\n--- {fmt_name} ---\")\n",
    "        start = time.time()\n",
    "        actions[\"save\"]()\n",
    "        save_time = time.time() - start\n",
    "        file_size = os.path.getsize(f\"test_graph.{fmt_name.lower()[:7]}\")\n",
    "        print(f\"Save time: {save_time:.2f}s\")\n",
    "        print(f\"File size: {file_size/1024/1024:.2f} MB\")\n",
    "        \n",
    "        # Test load\n",
    "        start = time.time()\n",
    "        loaded_kg = actions[\"load\"]()\n",
    "        load_time = time.time() - start\n",
    "        print(f\"Load time: {load_time:.2f}s\")\n",
    "        print(f\"Loaded {len(loaded_kg)} triples\")\n",
    "        \n",
    "        results[fmt_name] = {\n",
    "            \"save_time\": save_time,\n",
    "            \"load_time\": load_time,\n",
    "            \"file_size\": file_size,\n",
    "            \"triples\": len(loaded_kg)\n",
    "        }\n",
    "    \n",
    "    # Calculate relative performance\n",
    "    print(\"\\n--- Performance Comparison ---\")\n",
    "    base_fmt = \"Turtle\"\n",
    "    comp_fmt = \"Parquet\"\n",
    "    \n",
    "    save_speedup = results[base_fmt][\"save_time\"] / results[comp_fmt][\"save_time\"]\n",
    "    load_speedup = results[base_fmt][\"load_time\"] / results[comp_fmt][\"load_time\"]\n",
    "    size_reduction = results[base_fmt][\"file_size\"] / results[comp_fmt][\"file_size\"]\n",
    "    \n",
    "    print(f\"Save speedup: {save_speedup:.2f}x faster with {comp_fmt}\")\n",
    "    print(f\"Load speedup: {load_speedup:.2f}x faster with {comp_fmt}\")\n",
    "    print(f\"Size reduction: {size_reduction:.2f}x smaller with {comp_fmt}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run benchmark with different graph sizes\n",
    "benchmark_formats(graph_size=5000)  # Adjust based on your system's capacity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
